{
  "name": "Analytics Scrape Cron (Daily)",
  "nodes": [
    {
      "parameters": {
        "rule": {
          "interval": [
            {
              "field": "cronExpression",
              "expression": "0 3 * * *"
            }
          ]
        }
      },
      "type": "n8n-nodes-base.scheduleTrigger",
      "typeVersion": 1.2,
      "position": [-400, 300],
      "id": "cron-trigger",
      "name": "Daily 03:00 UTC"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "SELECT id, page_url, page_name, platform, user_id FROM company_pages WHERE is_active = true ORDER BY last_scraped_at ASC NULLS FIRST",
        "options": {}
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [-160, 300],
      "id": "get-active-pages",
      "name": "Get Active Pages",
      "credentials": {
        "postgres": {
          "id": "{{SUPABASE_POSTGRES_CREDENTIAL_ID}}",
          "name": "Supabase"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "const items = $input.all();\n\nif (!items || items.length === 0 || !items[0].json.id) {\n  return [{\n    json: {\n      no_pages: true,\n      message: 'No active company pages found'\n    }\n  }];\n}\n\nreturn items.map(item => ({\n  json: {\n    no_pages: false,\n    page_id: item.json.id,\n    page_url: item.json.page_url,\n    page_name: item.json.page_name,\n    platform: item.json.platform,\n    user_id: item.json.user_id\n  }\n}));"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [60, 300],
      "id": "prepare-pages",
      "name": "Prepare Pages"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "id": "check-no-pages",
              "leftValue": "={{ $json.no_pages }}",
              "rightValue": true,
              "operator": {
                "type": "boolean",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        }
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [280, 300],
      "id": "has-pages",
      "name": "Has Pages?"
    },
    {
      "parameters": {
        "jsCode": "// No active pages - nothing to do\nreturn [{ json: { message: 'No active company pages. Cron skipped.', timestamp: new Date().toISOString() } }];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [520, 180],
      "id": "no-pages-log",
      "name": "Log No Pages"
    },
    {
      "parameters": {
        "batchSize": 1,
        "options": {}
      },
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [520, 420],
      "id": "split-batches",
      "name": "Split In Batches"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO scrape_runs (company_page_id, user_id, status, started_at) VALUES ('{{ $json.page_id }}', '{{ $json.user_id }}', 'running', NOW()) RETURNING id",
        "options": {}
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [740, 420],
      "id": "create-scrape-run-cron",
      "name": "Create Scrape Run",
      "credentials": {
        "postgres": {
          "id": "{{SUPABASE_POSTGRES_CREDENTIAL_ID}}",
          "name": "Supabase"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "const runResult = $input.first();\nconst pageData = $('Split In Batches').first();\n\nreturn [{\n  json: {\n    scrape_run_id: runResult.json.id,\n    page_id: pageData.json.page_id,\n    page_url: pageData.json.page_url,\n    page_name: pageData.json.page_name,\n    platform: pageData.json.platform,\n    user_id: pageData.json.user_id\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [960, 420],
      "id": "merge-run-data-cron",
      "name": "Merge Run Data"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.firecrawl.dev/v1/scrape",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "httpHeaderAuth",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ JSON.stringify({ url: $json.page_url, formats: ['markdown'], waitFor: 3000 }) }}",
        "options": {
          "timeout": 60000
        }
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [1180, 420],
      "id": "firecrawl-scrape-cron",
      "name": "Firecrawl Scrape",
      "credentials": {
        "httpHeaderAuth": {
          "id": "{{FIRECRAWL_CREDENTIAL_ID}}",
          "name": "Firecrawl API"
        }
      },
      "retryOnFail": true,
      "maxTries": 2,
      "waitBetweenTries": 5000,
      "continueOnFail": true
    },
    {
      "parameters": {
        "jsCode": "const firecrawlResponse = $input.first();\nconst prevData = $('Merge Run Data').first();\n\n// Check if Firecrawl returned an error\nif (firecrawlResponse.json.error || !firecrawlResponse.json.data) {\n  const errorMsg = firecrawlResponse.json.error?.message || firecrawlResponse.json.message || 'Firecrawl scrape failed';\n  return [{\n    json: {\n      error: true,\n      errorMessage: errorMsg,\n      scrape_run_id: prevData.json.scrape_run_id,\n      page_id: prevData.json.page_id,\n      page_name: prevData.json.page_name,\n      user_id: prevData.json.user_id,\n      platform: prevData.json.platform,\n      posts: []\n    }\n  }];\n}\n\nconst markdown = firecrawlResponse.json.data.markdown || '';\nconst posts = [];\n\n// Split by common LinkedIn post delimiters\nconst postSections = markdown.split(/(?=#{1,3}\\s)|(?=---)|(?=\\*\\*\\*)|(?=\\n\\n(?:[A-Z][^\\n]{10,})\\n)/).filter(s => s.trim().length > 50);\n\nfor (let i = 0; i < postSections.length; i++) {\n  const section = postSections[i].trim();\n  \n  // Skip navigation/header sections\n  if (section.match(/^(Home|About|Posts|Jobs|People|Events|Videos|Sign|Log)/i)) continue;\n  if (section.length < 80) continue;\n  \n  // Extract post content\n  let content = section\n    .replace(/^#{1,3}\\s+.*\\n/, '')\n    .replace(/!\\[.*?\\]\\(.*?\\)/g, '')\n    .replace(/\\[([^\\]]+)\\]\\(([^)]+)\\)/g, '$1')\n    .replace(/\\*\\*/g, '')\n    .replace(/\\n{3,}/g, '\\n\\n')\n    .trim();\n  \n  if (content.length < 50) continue;\n  \n  // Extract engagement metrics\n  const reactionsMatch = section.match(/(\\d[\\d,.]*)\\s*(?:reactions?|likes?)/i);\n  const commentsMatch = section.match(/(\\d[\\d,.]*)\\s*comments?/i);\n  const sharesMatch = section.match(/(\\d[\\d,.]*)\\s*(?:shares?|reposts?)/i);\n  \n  const parseCount = (match) => {\n    if (!match) return 0;\n    return parseInt(match[1].replace(/[,.]/g, ''), 10) || 0;\n  };\n  \n  // Extract post URL\n  const urlMatch = section.match(/\\(https:\\/\\/www\\.linkedin\\.com\\/feed\\/update\\/[^)]+\\)/);\n  const postUrl = urlMatch ? urlMatch[0].slice(1, -1) : null;\n  \n  // Extract date\n  let postedAt = null;\n  const dateMatch = section.match(/(\\d{1,2}[dhmw]|\\d{1,2}\\s+(?:hours?|days?|weeks?|months?)\\s+ago|\\w+\\s+\\d{1,2},?\\s+\\d{4})/i);\n  if (dateMatch) {\n    const dateStr = dateMatch[1];\n    const now = new Date();\n    const daysMatch = dateStr.match(/(\\d+)d/);\n    const weeksMatch = dateStr.match(/(\\d+)w/);\n    const monthsMatch = dateStr.match(/(\\d+)mo/);\n    if (daysMatch) {\n      now.setDate(now.getDate() - parseInt(daysMatch[1]));\n      postedAt = now.toISOString();\n    } else if (weeksMatch) {\n      now.setDate(now.getDate() - parseInt(weeksMatch[1]) * 7);\n      postedAt = now.toISOString();\n    } else if (monthsMatch) {\n      now.setMonth(now.getMonth() - parseInt(monthsMatch[1]));\n      postedAt = now.toISOString();\n    } else {\n      try { postedAt = new Date(dateStr).toISOString(); } catch(e) {}\n    }\n  }\n  \n  // Detect media type\n  let mediaType = 'text';\n  if (section.match(/!\\[.*?\\]\\(.*?\\)/)) mediaType = 'image';\n  if (section.match(/video|watch|play/i)) mediaType = 'video';\n  if (section.match(/carousel|slides?|swipe/i)) mediaType = 'carousel';\n  \n  // Generate deterministic external_id\n  const hashSource = content.substring(0, 200);\n  let hash = 0;\n  for (let j = 0; j < hashSource.length; j++) {\n    const char = hashSource.charCodeAt(j);\n    hash = ((hash << 5) - hash) + char;\n    hash = hash & hash;\n  }\n  const externalId = 'li_' + Math.abs(hash).toString(36);\n  \n  // Clean content\n  content = content\n    .replace(/\\d[\\d,.]*\\s*(?:reactions?|likes?|comments?|shares?|reposts?).*$/gim, '')\n    .replace(/Like\\s*Comment\\s*Share.*/gi, '')\n    .replace(/\\n\\s*\\n\\s*\\n/g, '\\n\\n')\n    .trim();\n  \n  if (content.length < 50) continue;\n  \n  posts.push({\n    external_id: externalId,\n    content: content.substring(0, 5000),\n    post_url: postUrl,\n    posted_at: postedAt,\n    reactions_count: parseCount(reactionsMatch),\n    comments_count: parseCount(commentsMatch),\n    shares_count: parseCount(sharesMatch),\n    media_type: mediaType\n  });\n}\n\nreturn [{\n  json: {\n    error: false,\n    scrape_run_id: prevData.json.scrape_run_id,\n    page_id: prevData.json.page_id,\n    page_name: prevData.json.page_name,\n    user_id: prevData.json.user_id,\n    platform: prevData.json.platform,\n    posts: posts,\n    posts_found: posts.length\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1400, 420],
      "id": "parse-firecrawl-cron",
      "name": "Parse Firecrawl Response"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "id": "check-scrape-error-cron",
              "leftValue": "={{ $json.error }}",
              "rightValue": true,
              "operator": {
                "type": "boolean",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        }
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [1620, 420],
      "id": "scrape-ok-cron",
      "name": "Scrape OK?"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "UPDATE scrape_runs SET status = 'error', error_message = '{{ $json.errorMessage }}', completed_at = NOW() WHERE id = '{{ $json.scrape_run_id }}'",
        "options": {}
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [1840, 300],
      "id": "update-run-error-cron",
      "name": "Update Run Error",
      "credentials": {
        "postgres": {
          "id": "{{SUPABASE_POSTGRES_CREDENTIAL_ID}}",
          "name": "Supabase"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Log the error but continue to next page\nconst data = $input.first().json;\nconsole.log(`Scrape error for page ${$('Parse Firecrawl Response').first().json.page_name || $('Parse Firecrawl Response').first().json.page_id}: ${$('Parse Firecrawl Response').first().json.errorMessage}`);\n\nreturn [{ json: { continue: true } }];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [2060, 300],
      "id": "log-error-continue",
      "name": "Log Error & Continue"
    },
    {
      "parameters": {
        "jsCode": "const data = $input.first().json;\nconst posts = data.posts || [];\n\nif (posts.length === 0) {\n  return [{\n    json: {\n      ...data,\n      posts_new: 0,\n      posts_updated: 0,\n      upsert_complete: true\n    }\n  }];\n}\n\n// Build a single UPSERT query for all posts\nconst values = posts.map(p => {\n  const esc = (v) => v === null || v === undefined ? 'NULL' : `'${String(v).replace(/'/g, \"''\")}'`;\n  return `(${esc(data.page_id)}, ${esc(data.user_id)}, ${esc(data.platform)}, ${esc(p.external_id)}, ${esc(p.content)}, ${esc(p.post_url)}, ${p.posted_at ? esc(p.posted_at) : 'NULL'}, ${p.reactions_count || 0}, ${p.comments_count || 0}, ${p.shares_count || 0}, ${esc(p.media_type)})`;\n}).join(',\\n');\n\nconst query = `\nWITH upserted AS (\n  INSERT INTO scraped_posts (company_page_id, user_id, platform, external_id, content, post_url, posted_at, reactions_count, comments_count, shares_count, media_type)\n  VALUES ${values}\n  ON CONFLICT (company_page_id, external_id) DO UPDATE SET\n    content = EXCLUDED.content,\n    post_url = EXCLUDED.post_url,\n    posted_at = EXCLUDED.posted_at,\n    reactions_count = EXCLUDED.reactions_count,\n    comments_count = EXCLUDED.comments_count,\n    shares_count = EXCLUDED.shares_count,\n    media_type = EXCLUDED.media_type,\n    updated_at = NOW()\n  RETURNING id, (xmax = 0) AS is_new\n)\nSELECT\n  COUNT(*) AS total,\n  COUNT(*) FILTER (WHERE is_new) AS new_count,\n  COUNT(*) FILTER (WHERE NOT is_new) AS updated_count\nFROM upserted;\n`;\n\nreturn [{\n  json: {\n    ...data,\n    upsert_query: query\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1840, 540],
      "id": "build-upsert-query-cron",
      "name": "Build Upsert Query"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "id": "check-has-posts-cron",
              "leftValue": "={{ $json.upsert_complete }}",
              "rightValue": true,
              "operator": {
                "type": "boolean",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        }
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [2060, 540],
      "id": "has-posts-cron",
      "name": "Has Posts?"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "={{ $('Build Upsert Query').first().json.upsert_query }}",
        "options": {}
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [2280, 660],
      "id": "upsert-posts-cron",
      "name": "Upsert Posts",
      "credentials": {
        "postgres": {
          "id": "{{SUPABASE_POSTGRES_CREDENTIAL_ID}}",
          "name": "Supabase"
        }
      },
      "continueOnFail": true
    },
    {
      "parameters": {
        "jsCode": "const upsertResult = $input.first();\nconst prevData = $('Build Upsert Query').first().json;\n\nif (upsertResult.json.error) {\n  return [{\n    json: {\n      scrape_run_id: prevData.scrape_run_id,\n      page_id: prevData.page_id,\n      user_id: prevData.user_id,\n      posts_found: prevData.posts_found || 0,\n      posts_new: 0,\n      posts_updated: 0,\n      db_error: true,\n      errorMessage: upsertResult.json.error.message || 'Upsert failed'\n    }\n  }];\n}\n\nreturn [{\n  json: {\n    scrape_run_id: prevData.scrape_run_id,\n    page_id: prevData.page_id,\n    user_id: prevData.user_id,\n    posts_found: prevData.posts_found || 0,\n    posts_new: parseInt(upsertResult.json.new_count) || 0,\n    posts_updated: parseInt(upsertResult.json.updated_count) || 0,\n    db_error: false\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [2500, 660],
      "id": "process-upsert-result-cron",
      "name": "Process Upsert Result"
    },
    {
      "parameters": {
        "jsCode": "// No posts found for this page\nconst prevData = $input.first().json;\n\nreturn [{\n  json: {\n    scrape_run_id: prevData.scrape_run_id,\n    page_id: prevData.page_id,\n    user_id: prevData.user_id,\n    posts_found: 0,\n    posts_new: 0,\n    posts_updated: 0,\n    db_error: false\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [2280, 420],
      "id": "no-posts-result-cron",
      "name": "No Posts Result"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "UPDATE scrape_runs SET status = CASE WHEN {{ $json.db_error }} THEN 'error' ELSE 'success' END, posts_found = {{ $json.posts_found }}, posts_new = {{ $json.posts_new }}, posts_updated = {{ $json.posts_updated }}, error_message = CASE WHEN {{ $json.db_error }} THEN '{{ $json.errorMessage }}' ELSE NULL END, completed_at = NOW() WHERE id = '{{ $json.scrape_run_id }}'",
        "options": {}
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [2720, 540],
      "id": "update-run-success-cron",
      "name": "Update Scrape Run",
      "credentials": {
        "postgres": {
          "id": "{{SUPABASE_POSTGRES_CREDENTIAL_ID}}",
          "name": "Supabase"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "UPDATE company_pages SET last_scraped_at = NOW() WHERE id = '{{ $json.page_id || $('Process Upsert Result').first()?.json.page_id || $('No Posts Result').first()?.json.page_id }}'",
        "options": {}
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [2940, 540],
      "id": "update-last-scraped-cron",
      "name": "Update Last Scraped",
      "credentials": {
        "postgres": {
          "id": "{{SUPABASE_POSTGRES_CREDENTIAL_ID}}",
          "name": "Supabase"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Add a small delay between pages to avoid rate limiting\nconst delay = ms => new Promise(resolve => setTimeout(resolve, ms));\nawait delay(2000);\n\nreturn [{ json: { continue: true } }];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [3160, 420],
      "id": "rate-limit-delay",
      "name": "Rate Limit Delay"
    }
  ],
  "connections": {
    "Daily 03:00 UTC": {
      "main": [
        [
          {
            "node": "Get Active Pages",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Get Active Pages": {
      "main": [
        [
          {
            "node": "Prepare Pages",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Pages": {
      "main": [
        [
          {
            "node": "Has Pages?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Has Pages?": {
      "main": [
        [
          {
            "node": "Log No Pages",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Split In Batches",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Split In Batches": {
      "main": [
        [
          {
            "node": "Create Scrape Run",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Create Scrape Run": {
      "main": [
        [
          {
            "node": "Merge Run Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge Run Data": {
      "main": [
        [
          {
            "node": "Firecrawl Scrape",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Firecrawl Scrape": {
      "main": [
        [
          {
            "node": "Parse Firecrawl Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse Firecrawl Response": {
      "main": [
        [
          {
            "node": "Scrape OK?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Scrape OK?": {
      "main": [
        [
          {
            "node": "Update Run Error",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Build Upsert Query",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Update Run Error": {
      "main": [
        [
          {
            "node": "Log Error & Continue",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Log Error & Continue": {
      "main": [
        [
          {
            "node": "Rate Limit Delay",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build Upsert Query": {
      "main": [
        [
          {
            "node": "Has Posts?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Has Posts?": {
      "main": [
        [
          {
            "node": "No Posts Result",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Upsert Posts",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Upsert Posts": {
      "main": [
        [
          {
            "node": "Process Upsert Result",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Process Upsert Result": {
      "main": [
        [
          {
            "node": "Update Scrape Run",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "No Posts Result": {
      "main": [
        [
          {
            "node": "Update Scrape Run",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Update Scrape Run": {
      "main": [
        [
          {
            "node": "Update Last Scraped",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Update Last Scraped": {
      "main": [
        [
          {
            "node": "Rate Limit Delay",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Rate Limit Delay": {
      "main": [
        [
          {
            "node": "Split In Batches",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1",
    "availableInMCP": false
  },
  "versionId": "",
  "meta": {
    "instanceId": ""
  },
  "id": "",
  "tags": []
}